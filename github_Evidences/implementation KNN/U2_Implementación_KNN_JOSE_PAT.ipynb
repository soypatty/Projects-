{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# K-Nearest Neighbors Algorithm"
      ],
      "metadata": {
        "id": "6_g4r6W6f16z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is a non-parametric supervised learning classifier, which uses proximity to make classifications or predictions about the clustering of an individual data point. While it can be used for regression or classification problems, it is typically used as a classification algorithm, assuming that similar points can be found close to each other.\n"
      ],
      "metadata": {
        "id": "R2Y7MScsh28r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# intuition behind the KNN algorithm."
      ],
      "metadata": {
        "id": "a6VEg_Zu3vqT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For classification problems, a class label is assigned on the basis of a majority vote—i.e. the label that is most frequently represented around a given data point is used. While this is technically considered “plurality voting”, the term, “majority vote” is more commonly used in literature. The distinction between these terminologies is that “majority voting” technically requires a majority of greater than 50%, which primarily works when there are only two categories. When you have multiple classes—e.g. four categories, you don’t necessarily need 50% of the vote to make a conclusion about a class; you could assign a class label with a vote of greater than 25%.\n",
        "\n",
        "Regression problems use a similar concept as classification problem, but in this case, the average the k nearest neighbors is taken to make a prediction about a classification. The main distinction here is that classification is used for discrete values, whereas regression is used with continuous ones. However, before a classification can be made, the distance must be defined. Euclidean distance is most commonly used."
      ],
      "metadata": {
        "id": "ZWFYpoIN3-FS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# pseudocode"
      ],
      "metadata": {
        "id": "J9CcdfiwCuvO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. KNN algorithm inputs:\n",
        "\n",
        "X_train: Training data set, consisting of a feature matrix and a label matrix.\n",
        "\n",
        "y_train: Array of labels of the training data set.\n",
        "\n",
        "X_test: Test data set, consisting of a feature matrix.\n",
        "\n",
        "k: Number of nearest neighbors to consider.\n",
        "\n",
        "2. Output\n",
        "\n",
        "The output of the KNN algorithm is a prediction matrix for the test data set.\n",
        "\n",
        "3. Algorithm\n",
        "\n",
        "The KNN algorithm can be divided into the following steps:\n",
        "\n",
        "1.   Calculate the distances between the test and training data points. This step can be performed using any distance measure, such as the Euclidean distance, Manhattan distance, or Mahalanobis distance.\n",
        "2.   Find the k nearest neighbors for each test data point. This step can be performed using any search method, such as bubble sort, selection sort, or insertion sort.\n",
        "3.   Assign nearest neighbor labels to each test data point. This step can be done using the majority vote, the average, or the mean.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mnRbCRXQCrAX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#implementation\n"
      ],
      "metadata": {
        "id": "j6MGq-SfWpck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary libraries\n",
        "import numpy as np # this is used for numerical operations\n",
        "import pandas as pd # Pandas simplifies data manipulation and analysis\n",
        "from sklearn.model_selection import train_test_split # Scikit-learn is a machine learning library, and 'train_test_split' is used to split the dataset into training and testing sets\n",
        "from sklearn.neighbors import KNeighborsClassifier # Scikit-learn's 'KNeighborsClassifier' provides an implementation of the k-nearest neighbors algorithm for classification\n",
        "from sklearn.metrics import accuracy_score # 'accuracy_score' from Scikit-learn metrics is used to measure the accuracy of the model's predictions\n",
        "\n"
      ],
      "metadata": {
        "id": "CNXU1yZ9_2jb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42) # Sets the seed of the NumPy random number generator to 42."
      ],
      "metadata": {
        "id": "2KGzZKjnVtYS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = {\n",
        "    'Rojo': np.random.randint(0, 256, 100),\n",
        "    'Verde': np.random.randint(0, 256, 100),\n",
        "    'Azul': np.random.randint(0, 256, 100),\n",
        "    # Generate lists of 100 random integers between 0 and 255 to represent red color intensity for the toys\n",
        "    'Juguete': ['Carro' if i < 50 else 'Muñeca' for i in range(100)]  # Create a list of 100 elements, where the first 50 are 'Carro' and the next 50 are 'Muñeca'\n",
        "}\n"
      ],
      "metadata": {
        "id": "1_vfC-mKVwIj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " These generate simulated data for a set of toys with color attributes (red, green, blue) and a label indicating whether it is a car or a doll."
      ],
      "metadata": {
        "id": "vAZGDGBSW84f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(data) # Create a DataFrame named 'df' using the previously generated 'data'\n",
        "X_train, X_test, y_train, y_test = train_test_split(df[['Rojo', 'Verde', 'Azul']], df['Juguete'], test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "k2P7mqIEpYEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split into training and test sets (train_test_split): Splits the DataFrame into training (X_train, y_train) and test (X_test, y_test) sets."
      ],
      "metadata": {
        "id": "9RQa0SUBqYMj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "k = 3  # Number of neighbors (defined by us)\n",
        "knn = KNeighborsClassifier(n_neighbors=k)  # Create a k-NN classifier with k neighbors\n",
        "knn.fit(X_train, y_train)  # Train the model using the training data\n",
        "nuevo_juguete = pd.DataFrame({\n",
        "    'Rojo': [150],\n",
        "    'Verde': [30],\n",
        "    'Azul': [60]\n",
        "})\n"
      ],
      "metadata": {
        "id": "JDCrcp9nqh-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a prediction for the new toy\n",
        "prediccion = knn.predict(nuevo_juguete)\n",
        "print(f'The predicted toy type is: {prediccion[0]}')\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = knn.predict(X_test)\n",
        "\n",
        "# Calculate the model accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DxzIkTlUH8wr",
        "outputId": "bb84d505-ae6a-44de-e0c1-0ba93ced3a9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The predicted toy type is: Muñeca\n",
            "Accuracy: 0.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "k\n",
        " -NN does not have a loss function that can be minimized during training. In fact, this algorithm is not trained at all. The only \"training\" that happens for k\n",
        "-NN, is memorising the data (creating a local copy), so that during prediction you can do a search and majority vote. Technically, no function is fitted to the data, and so, no optimization is done (it cannot be trained using gradient descent).\n",
        "\n",
        "https://stats.stackexchange.com/questions/420416/does-knn-have-a-loss-function"
      ],
      "metadata": {
        "id": "aaIqDbgUsPXZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It doesn't involve optimization of model parameters through iterative updates like gradient descent in neural networks or genetic algorithms. Instead, it classifies or predicts new data points based on their similarity to existing data points.\n",
        "\n",
        "https://medium.com/@denizgunay/knn-algorithm-3604c19cd809#:~:text=%E2%96%B9KNN%20is%20a%20simple,similarity%20to%20existing%20data%20points."
      ],
      "metadata": {
        "id": "TjaVVt_BtNjY"
      }
    }
  ]
}